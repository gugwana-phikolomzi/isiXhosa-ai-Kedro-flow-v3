model_A2:
  experiment_id: "A2"
  tokenizer: { type: "byte", add_special_tokens: true }
  train_frac: 0.9
  block_size: 224

  n_layer: 4
  n_head: 4
  n_embd: 256
  dropout: 0.1

  batch_size: 1
  grad_accum_steps: 7
  max_iters: 20000
  target_total_steps: null

  eval_interval: 250
  eval_iters: 40
  sample_tokens: 150
  sample_prompts: ["Molo ", "Ulwimi lwesiXhosa "]

  learning_rate: 0.0006
  weight_decay: 0.20
  grad_clip_norm: 1.0

  lr_schedule: "cosine"
  warmup_steps: 200
  min_lr: 0.00001

  device_auto_cuda: false
  torch_threads: 6
  checkpoint_interval: 500
  checkpoint_dir: "data/06_models/A2/checkpoints"
  checkpoint_prefix: "A2"

  resume: { enabled: false, path: null }

  metrics_jsonl_path: "data/08_reporting/A2/metrics.jsonl"
  metrics_csv_path:  "data/08_reporting/A2/metrics.csv"

  early_stopping: { enabled: true, patience_evals: 5, min_improve_pct: 1.0, min_delta_abs: 0.0 }
  seed: 1337
