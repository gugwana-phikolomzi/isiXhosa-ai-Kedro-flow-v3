{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FAST SAMPLER (no Kedro), hard-coded artifacts & decoder ====\n",
    "from pathlib import Path\n",
    "import os, sys, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Optional: avoid slow CUDA probing for a quick CPU sampler\n",
    "# os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "# os.environ.setdefault(\"PYTHONUNBUFFERED\", \"1\")\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0) Project root discovery (so this runs fine from notebooks/ too)\n",
    "# ---------------------------------------------------------------------------\n",
    "def find_project_root(start: Path = Path.cwd()) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(f\"Couldn't find pyproject.toml above {start}\")\n",
    "\n",
    "PROJECT_PATH = find_project_root()\n",
    "src_path = PROJECT_PATH / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "print(\"Project root:\", PROJECT_PATH)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Choose model + casing\n",
    "# ---------------------------------------------------------------------------\n",
    "# Set one of: \"model_B21\" (CASED) or \"model_B22\" (CAPS)\n",
    "MODEL_ID  = \"model_B21\"\n",
    "\n",
    "# If you prefer to infer from MODEL_ID, set CASE_MODE=None; otherwise force \"CASED\" or \"CAPS\"\n",
    "CASE_MODE = None  # e.g., \"CASED\" or \"CAPS\" to override\n",
    "\n",
    "def _infer_case_mode(model_id: str) -> str:\n",
    "    if CASE_MODE in (\"CASED\", \"CAPS\"):\n",
    "        return CASE_MODE\n",
    "    # Heuristic: CAPS for B22, CASED otherwise\n",
    "    return \"CAPS\" if model_id.endswith(\"B22\") or model_id.endswith(\"_B22\") else \"CASED\"\n",
    "\n",
    "CASE_MODE = _infer_case_mode(MODEL_ID)\n",
    "print(f\"CASE_MODE: {CASE_MODE}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2) Hard-code artifact locations (no recursive scans, no metrics lookup)\n",
    "# ---------------------------------------------------------------------------\n",
    "CKPT_DIR = PROJECT_PATH / f\"data/06_models/{MODEL_ID}\"\n",
    "TOK_DIR  = CKPT_DIR  # prefer colocated tokenizer spec\n",
    "\n",
    "# Choose the first existing checkpoint from this fixed, small list\n",
    "_ckpt_candidates = [\n",
    "    CKPT_DIR / \"state_dict_best.pt\",\n",
    "    CKPT_DIR / \"best.pt\",\n",
    "    CKPT_DIR / \"model_best.pt\",\n",
    "    CKPT_DIR / \"state_dict.pt\",          # fallback\n",
    "]\n",
    "CKPT_PATH = next((p for p in _ckpt_candidates if p.exists()), None)\n",
    "if CKPT_PATH is None:\n",
    "    raise FileNotFoundError(f\"No checkpoint found. Tried:\\n\" + \"\\n\".join(str(p) for p in _ckpt_candidates))\n",
    "print(\"Using checkpoint:\", CKPT_PATH)\n",
    "\n",
    "# Tokenizer spec: prefer colocated, then a single fixed fallback\n",
    "_tok_candidates = [\n",
    "    TOK_DIR / \"tokenizer_spec.json\",\n",
    "    PROJECT_PATH / f\"data/02_intermediate/{MODEL_ID}/tokenizer_spec.json\",\n",
    "]\n",
    "TOK_PATH = next((p for p in _tok_candidates if p.exists()), None)\n",
    "if TOK_PATH is None:\n",
    "    raise FileNotFoundError(f\"tokenizer_spec.json not found. Tried:\\n\" + \"\\n\".join(str(p) for p in _tok_candidates))\n",
    "print(\"Using tokenizer spec:\", TOK_PATH)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3) Load artifacts\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Loading checkpoint…\")\n",
    "try:\n",
    "    state_dict = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=True)  # PyTorch 2.x\n",
    "except TypeError:\n",
    "    state_dict = torch.load(CKPT_PATH, map_location=\"cpu\")                      # older PyTorch\n",
    "print(\"✓ checkpoint loaded\")\n",
    "\n",
    "print(\"Loading tokenizer spec…\")\n",
    "with open(TOK_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenizer_spec = json.load(f)\n",
    "print(\"✓ tokenizer spec loaded\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4) Hard-code the decoder import\n",
    "#    Change this line if your decode() lives somewhere else.\n",
    "# ---------------------------------------------------------------------------\n",
    "from main.pipelines.model_common.inference import decode\n",
    "print(\"Using decoder: main.pipelines.model_common.inference.decode()\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5) Sampler config + helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "TOP_P, TOP_K, STOP = 0.90, 50, None\n",
    "\n",
    "def _maybe_caps(s: str) -> str:\n",
    "    return s.upper() if CASE_MODE == \"CAPS\" and isinstance(s, str) else s\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6) Generate 4 random samples\n",
    "# ---------------------------------------------------------------------------\n",
    "pairs = [(0.7, 111), (0.9, 222), (0.7, 333), (0.9, 444)]\n",
    "rows = []\n",
    "prefix = _maybe_caps(\"\")\n",
    "for i, (t, s) in enumerate(pairs, 1):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    text = decode(\n",
    "        state_dict, tokenizer_spec, prefix,\n",
    "        max_new_tokens=120, top_p=TOP_P, top_k=TOP_K,\n",
    "        temperature=float(t), seed=int(s), stop=STOP\n",
    "    )\n",
    "    rows.append({\n",
    "        \"kind\":\"random\",\"idx\":i,\"prompt\":prefix,\n",
    "        \"temperature\":t,\"seed\":s,\n",
    "        \"top_p\":TOP_P,\"top_k\":TOP_K,\"max_new_tokens\":120,\n",
    "        \"text\":text\n",
    "    })\n",
    "df_random = pd.DataFrame(rows)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 7) Generate 4 prompted samples\n",
    "# ---------------------------------------------------------------------------\n",
    "prompts = [\n",
    "    \"Bhala isiqendu esifutshane sichaza inkqubo yokuvota eMzantsi Afrika.\",\n",
    "    \"Qalisa ibali: 'Kwizolo kusasa, ndiphume ndisiya eTaxi Rank...'\",\n",
    "    \"Phendula nge-JSON enezitshixo `topic`, `bullets` (3) ngomxholo: 'Ukulungiselela udliwanondlebe lomsebenzi'.\",\n",
    "    \"Nika uluhlu lwezixeko ezi-5 eMpuma Koloni ngesiXhosa, uze uchaze esinye ngesiNgesi kwisivakalisi esinye.\",\n",
    "]\n",
    "prompts = [_maybe_caps(p) for p in prompts]\n",
    "\n",
    "rows = []\n",
    "T, S = 0.7, 333\n",
    "for i, pr in enumerate(prompts, 1):\n",
    "    random.seed(S); np.random.seed(S); torch.manual_seed(S)\n",
    "    text = decode(\n",
    "        state_dict, tokenizer_spec, pr,\n",
    "        max_new_tokens=120, top_p=TOP_P, top_k=TOP_K,\n",
    "        temperature=float(T), seed=int(S), stop=STOP\n",
    "    )\n",
    "    rows.append({\n",
    "        \"kind\":\"prompted\",\"idx\":i,\"prompt\":pr,\n",
    "        \"temperature\":T,\"seed\":S,\n",
    "        \"top_p\":TOP_P,\"top_k\":TOP_K,\"max_new_tokens\":120,\n",
    "        \"text\":text\n",
    "    })\n",
    "df_prompt = pd.DataFrame(rows)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 8) Save CSVs where the reporting pipeline expects\n",
    "# ---------------------------------------------------------------------------\n",
    "out_dir = PROJECT_PATH / \"data/08_reporting\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "rand_path = out_dir / \"_random_samples.csv\"\n",
    "prpt_path = out_dir / \"_prompted_samples.csv\"\n",
    "df_random.to_csv(rand_path, index=False)\n",
    "df_prompt.to_csv(prpt_path, index=False)\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"  -\", rand_path)\n",
    "print(\"  -\", prpt_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
